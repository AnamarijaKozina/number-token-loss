{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:44:39.774801Z",
     "start_time": "2024-07-07T18:44:36.642202Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "### Huggingface dataset and tokenizer imports\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "from src.tokenizer.xval_tokenizer import XvalTokenizer\n",
    "from src.transformer_backbone.xlnet import XLNetBackbone\n",
    "from src.encoding_decoding.xval_encoding_decoding import XValModel, define_masked_num_collator as xval_define_masked_num_collator\n",
    "\n",
    "from src.tokenizer.rt_tokenizer import RtTokenizer\n",
    "from src.encoding_decoding.rt_encoding_decoding import RegressionTransformer, define_masked_num_collator as rt_define_masked_num_collator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555da0f1d7561456",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972c6bb08541c62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:44:51.544532Z",
     "start_time": "2024-07-07T18:44:51.342216Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function to read the text file and yield examples\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for i in range(0, len(lines), 2):\n",
    "        question = lines[i].strip()\n",
    "        answer = lines[i+1].strip()\n",
    "        yield {'question': question, 'answer': answer}\n",
    "\n",
    "# Define the dataset loading function\n",
    "def load_txt_dataset(file_path):\n",
    "    return Dataset.from_generator(read_txt, gen_kwargs={'file_path': file_path})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a43a0143e570d4f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:44:52.367977Z",
     "start_time": "2024-07-07T18:44:52.165857Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 117\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = './data/mathematics_dataset-v1.0/mathematics_dataset-v1.0/train-easy/algebra__linear_1d_small.txt'\n",
    "\n",
    "ds = load_txt_dataset(data_path)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eeb30d4c54bafe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:44:53.898916Z",
     "start_time": "2024-07-07T18:44:53.687811Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Solve 0 = 4*b + b + 15 for b.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"question\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86262376ecabca53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:44:54.975602Z",
     "start_time": "2024-07-07T18:44:54.752247Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"answer\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bc3538d10a02c9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load pretrained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aef0cc1850ecd0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:45:01.201049Z",
     "start_time": "2024-07-07T18:44:57.916088Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlnet-base-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m pretrained_tokenizer \u001b[38;5;241m=\u001b[39m XLNetTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 3\u001b[0m transformer_backbone \u001b[38;5;241m=\u001b[39m XLNetBackbone(model_name)\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/miniconda3/envs/i2dl/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    899\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \n\u001b[1;32m    901\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mcuda(device))\n",
      "File \u001b[0;32m~/miniconda3/envs/i2dl/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/i2dl/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/i2dl/lib/python3.11/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/i2dl/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    899\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \n\u001b[1;32m    901\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mcuda(device))\n",
      "File \u001b[0;32m~/miniconda3/envs/i2dl/lib/python3.11/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    292\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 293\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_init()\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    297\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "model_name = \"xlnet-base-cased\"\n",
    "pretrained_tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "transformer_backbone = XLNetBackbone(model_name).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf788e466c764",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Xval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ff4eaf104d230a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:45:02.426583Z",
     "start_time": "2024-07-07T18:45:02.301065Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number token IDs: [32000]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = XvalTokenizer(\n",
    "    special_tokens={\n",
    "            \"additional_special_tokens\": [\"[NUM]\"],\n",
    "            \"pad_token\": \"[PAD]\",\n",
    "            \"mask_token\": \"[MASK]\",\n",
    "        },\n",
    "    num_tokens=[\"[NUM]\"], # not explicitly added to tokenizer! Has to be specified in special_tokens above\n",
    "    embedding_dim=128,# TODO\n",
    "    pretrained_tokenizer=pretrained_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75805a22e73ef981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:45:03.436001Z",
     "start_time": "2024-07-07T18:45:03.253708Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_x = tokenizer(ds[0][\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e65bb8917b18364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:45:04.581141Z",
     "start_time": "2024-07-07T18:45:04.431137Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve 0 = 4*b + b + 15 for b.\n",
      "['Sol', 've', '[NUM]', '=', '[NUM]', '*', 'b', '+', '', 'b', '+', '[NUM]', 'for', '', 'b', '.', '<sep>', '<cls>']\n",
      "[1.0, 1.0, 0.0, 1.0, 4.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 15.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(ds[0][\"question\"])\n",
    "print([tokenizer.tokenizer.decode(x) for x in tokenized_x[\"input_ids\"]])\n",
    "print([x for x in tokenized_x[\"numbers\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb82958cbc325764",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:45:06.227585Z",
     "start_time": "2024-07-07T18:45:05.902582Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting tokenization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f31d06309a4a5e8e00020c6b1c99fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nStarting tokenization...\")\n",
    "tokenize_lambda = lambda x: {\"question\": tokenizer(x[\"question\"]), \"answer\": tokenizer(x[\"answer\"])}\n",
    "tokenized_ds = ds.map(\n",
    "    tokenize_lambda,\n",
    "    batched=False,\n",
    "    # num_proc=30,\n",
    "    load_from_cache_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f20637e1e61277ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:45:18.817677Z",
     "start_time": "2024-07-07T18:45:18.116404Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer_backbone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m XValModel(transformer_backbone\u001b[38;5;241m=\u001b[39mtransformer_backbone, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mtokenizer), dim_feedforward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1536\u001b[39m, context_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m955\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      3\u001b[0m pad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m      4\u001b[0m mask_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmask_token_id\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformer_backbone' is not defined"
     ]
    }
   ],
   "source": [
    "model = XValModel(transformer_backbone=transformer_backbone, vocab_size=len(tokenizer.tokenizer), dim_feedforward=1536, context_length=955).cuda()\n",
    "\n",
    "pad_token_id = tokenizer.tokenizer.pad_token_id\n",
    "mask_token_id = tokenizer.tokenizer.mask_token_id\n",
    "mlm_probability = 0.3\n",
    "epochs = 10\n",
    "lr = 1e-4\n",
    "weight_decay = 0.01\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "collator = xval_define_masked_num_collator(pad_token_id, mask_token_id, mlm_probability)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_ds[\"question\"],\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76f62915627ec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:46:38.610557Z",
     "start_time": "2024-07-07T18:45:19.860560Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:06<00:57,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0: loss_mlm = 11.248; loss_num = 51407.855; loss_total = 51419.104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:13<00:55,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: loss_mlm = 11.126; loss_num = 81999.289; loss_total = 82010.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:22<00:53,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: loss_mlm = 10.889; loss_num = 82899.489; loss_total = 82910.380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:30<00:46,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: loss_mlm = 10.618; loss_num = 82658.826; loss_total = 82669.446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:39<00:42,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: loss_mlm = 10.362; loss_num = 124809.849; loss_total = 124820.213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:48<00:34,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: loss_mlm = 10.089; loss_num = 159113.855; loss_total = 159123.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:56<00:24,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: loss_mlm = 9.815; loss_num = 156969.954; loss_total = 156979.772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:03<00:16,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: loss_mlm = 9.553; loss_num = 158357.218; loss_total = 158366.772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:11<00:07,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: loss_mlm = 9.294; loss_num = 152996.410; loss_total = 153005.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:18<00:00,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: loss_mlm = 9.044; loss_num = 149240.996; loss_total = 149250.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "loss_mlm_hist = []\n",
    "loss_num_hist = []\n",
    "\n",
    "max_n_batches = 100\n",
    "\n",
    "try:\n",
    "    for e in tqdm(range(epochs)):\n",
    "        n_batches = 0\n",
    "        for batch in train_loader:\n",
    "            if n_batches > max_n_batches:\n",
    "                break\n",
    "            logit_preds, num_preds = model(\n",
    "                batch[\"x\"].cuda(),\n",
    "                batch[\"x_num\"].cuda(),\n",
    "                batch[\"attention_mask\"].cuda(),\n",
    "                batch[\"token_type_ids\"].cuda(),\n",
    "            )\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                loss_mlm = F.cross_entropy(\n",
    "                    logit_preds.view(-1, logit_preds.size(-1)),\n",
    "                    batch[\"y\"].cuda().view(-1),\n",
    "                    ignore_index=-100,\n",
    "                    reduction=\"mean\",\n",
    "                )\n",
    "                num_mask = batch['y'] == tokenizer.tokenizer.convert_tokens_to_ids(\"[NUM]\")\n",
    "                loss_num = F.mse_loss(\n",
    "                    num_preds[num_mask],\n",
    "                    batch[\"y_num\"][num_mask].view(-1, 1).cuda(),\n",
    "                    reduction=\"mean\",\n",
    "                )\n",
    "            loss = loss_mlm + loss_num\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_hist.append(loss.item())\n",
    "            loss_mlm_hist.append(loss_mlm.item())\n",
    "            loss_num_hist.append(loss_num.item())\n",
    "            n_batches += 1\n",
    "\n",
    "            try:\n",
    "                loss_avg = 0.99 * loss_avg + 0.01 * loss.item()\n",
    "                loss_mlm_avg = 0.99 * loss_mlm_avg + 0.01 * loss_mlm.item()\n",
    "                loss_num_avg = 0.99 * loss_num_avg + 0.01 * loss_num.item()\n",
    "            except:\n",
    "                loss_avg = loss.item()\n",
    "                loss_mlm_avg = loss_mlm.item()\n",
    "                loss_num_avg = loss_num.item()\n",
    "\n",
    "        checkpoint = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"loss\": loss_avg,\n",
    "            \"loss_hist\": loss_hist,\n",
    "            \"loss_mlm_hist\": loss_mlm_hist,\n",
    "            \"loss_num_hist\": loss_num_hist,\n",
    "        }\n",
    "        torch.save(checkpoint, \"./ckpt.pt\")\n",
    "        print(f\"Epoch #{e}: loss_mlm = {loss_mlm_avg:.3f}; loss_num = {loss_num_avg:.3f}; loss_total = {loss_avg:.3f}\")\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b217ca79fdbf3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Regression Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672989b5106f7d13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:50:19.751330Z",
     "start_time": "2024-07-07T18:50:19.518014Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_num_tokens(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "num_tokens = read_num_tokens(\"regression_transformer_number_tokens.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ec783ffb8b291",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:50:20.346729Z",
     "start_time": "2024-07-07T18:50:20.194606Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_0_0_', '_1_0_', '_2_0_', '_3_0_', '_4_0_']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a628e81c204ef57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:50:21.026171Z",
     "start_time": "2024-07-07T18:50:20.882772Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = RtTokenizer(pretrained_tokenizer, num_tokens, embedding_dim=transformer_backbone.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7b66f6dc69c4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:50:21.791106Z",
     "start_time": "2024-07-07T18:50:21.631155Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve 0 = 4*b + b + 15 for b.\n"
     ]
    }
   ],
   "source": [
    "print(ds[0][\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce7bbb3a3f2821",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:50:22.818744Z",
     "start_time": "2024-07-07T18:50:22.614624Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve 0 = 4*b + b + 15 for b.\n",
      "['Sol', 've', '_0_0_', '=', '_4_0_', '*', 'b', '+', '', 'b', '+', '_1_1_', '_5_0_', 'for', '', 'b', '.', '<sep>', '<cls>']\n",
      "[0.0, 0.0, 0.0, 0.0, 2.7699864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.9249663, 3.4624832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "tokenized_x = tokenizer(ds[0][\"question\"])\n",
    "print(ds[0][\"question\"])\n",
    "print([tokenizer.tokenizer.decode(x) for x in tokenized_x[\"input_ids\"]])\n",
    "print([x.sum() for x in tokenized_x[\"number_embeddings\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423fea9a7bd46a45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:50:26.555983Z",
     "start_time": "2024-07-07T18:50:25.826365Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting tokenization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406476198d0b48c8b30eb2ae0f58c3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nStarting tokenization...\")\n",
    "tokenize_lambda = lambda x: {\"question\": tokenizer(x[\"question\"]), \"answer\": tokenizer(x[\"answer\"])}\n",
    "tokenized_ds = ds.map(\n",
    "    tokenize_lambda,\n",
    "    batched=False,\n",
    "    # num_proc=30,\n",
    "    load_from_cache_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac466b2397ff20ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:50:28.589883Z",
     "start_time": "2024-07-07T18:50:27.075860Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RegressionTransformer(transformer_backbone=transformer_backbone, vocab_size=len(tokenizer.tokenizer), dim_feedforward=1536,\n",
    "                  context_length=955).cuda()\n",
    "\n",
    "pad_token_id = tokenizer.tokenizer.pad_token_id\n",
    "mask_token_id = tokenizer.tokenizer.mask_token_id\n",
    "mlm_probability = 0.3\n",
    "epochs = 10\n",
    "lr = 1e-4\n",
    "weight_decay = 0.01\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "collator = rt_define_masked_num_collator(pad_token_id, mask_token_id, mlm_probability)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_ds[\"question\"],\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75124d4f0b53ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T18:53:11.737055Z",
     "start_time": "2024-07-07T18:50:29.190812Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:19<02:59, 19.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0: loss = 143369.369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:36<02:25, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: loss = 137720.320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:56<02:10, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: loss = 132293.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:13<01:47, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: loss = 127081.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:27<01:23, 16.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: loss = 122073.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:49<01:13, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: loss = 117263.670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:02<00:49, 16.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: loss = 112643.155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:16<00:31, 15.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: loss = 108204.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:29<00:14, 14.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: loss = 103941.141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:42<00:00, 16.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: loss = 99845.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "\n",
    "max_n_batches = 100\n",
    "\n",
    "try:\n",
    "    for e in tqdm(range(epochs)):\n",
    "        n_batches = 0\n",
    "        for batch in train_loader:\n",
    "            if n_batches > max_n_batches:\n",
    "                break\n",
    "            logit_preds = model(\n",
    "                batch[\"x\"].cuda(),\n",
    "                batch[\"number_embeddings\"].cuda(),\n",
    "                batch[\"attention_mask\"].cuda(),\n",
    "                batch[\"token_type_ids\"].cuda(),\n",
    "            )\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                loss = F.cross_entropy(\n",
    "                    logit_preds.view(-1, logit_preds.size(-1)),\n",
    "                    batch[\"y\"].cuda().view(-1),\n",
    "                    ignore_index=-100,\n",
    "                    reduction=\"mean\",\n",
    "                )\n",
    "               \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_hist.append(loss.item())\n",
    "            n_batches += 1\n",
    "\n",
    "            try:\n",
    "                loss_avg = 0.99 * loss_avg + 0.01 * loss.item()\n",
    "            except:\n",
    "                loss_avg = loss.item()\n",
    "\n",
    "        checkpoint = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"loss\": loss_avg,\n",
    "            \"loss_hist\": loss_hist,\n",
    "        }\n",
    "        torch.save(checkpoint, \"./ckpt.pt\")\n",
    "        print(f\"Epoch #{e}: loss = {loss_avg:.3f}\")\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b064a020a15d1f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
